# -*- coding: utf-8 -*-
"""Book Recommendation with Content Based Filtering and Collaborative Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pfKAiN1AwJZtMeksV1ErzHSSJiKrXFIA

# **Book Recommender System: Content Based Filtering**

# **Data Understanding**
"""

pip install surprise

# Prepare all packages
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import cross_validate
pd.set_option('mode.chained_assignment', None)
pd.set_option('display.max_colwidth', None)

# Unzip data
!unzip /content/buku.zip

# Load Data
pengguna = pd.read_csv('/content/Users.csv')
nilai = pd.read_csv('/content/Ratings.csv')
buku = pd.read_csv('/content/Books.csv')

# Cek Shape
print(pengguna.shape)
print(nilai.shape)
print(buku.shape)

"""### **Eksplorasi Dataset Buku**"""

# Cek info data buku
print(buku.shape)
buku.columns=['ISBN','Title','Author','Year_Of_Publication','Publisher','Image_URL_S','Image_URL_M','Image_URL_L']
buku.drop(['Image_URL_S','Image_URL_L'],axis=1,inplace=True)
buku.head()

# Cek Missing Value
B = ((buku.isnull().sum()).sort_values()).to_dict()
for x in B:
  print(x, ":", B[x])

# Cek info data buku
buku.info()

# Cek data duplikat pada rows
duplicateRowsB = buku[buku.duplicated()]
duplicateRowsB.shape

# Cek unique values dari ISBN dan title
print('unique ISBN: ', len(buku['ISBN'].unique()))
print('total rows: ', buku.shape[0])
print('unique title: ', len(buku['Title'].unique()))
print('total row: ', buku.shape[0])

"""Beberapa judul mengalami perulangan karena kuantitasnya tidak match dengan row"""

# Handle Missing Value dengan fillna
buku['Author'].fillna('Unknown', inplace=True)
buku['Publisher'].fillna('Unknown', inplace=True)
buku.isnull().sum()

# Visualisasi Top 7 Publisher

my_lib = (buku['Publisher'].value_counts()).to_dict()
count = pd.DataFrame(list(my_lib.items()), columns=['c','count'])
A = count.sort_values(by=['count'], ascending=False)
A.head()

label = 'Harlequin', 'Silhouette', 'Pocket', 'Ballantine Books', 'Bantam Books','Scholastic','Simon & Schuster', 'Penguin Books', 'Berkley Publishing Group', 'Warner Books'
size = [count['count'].iloc[0],
        count['count'].iloc[1],
        count['count'].iloc[2],
        count['count'].iloc[3],
        count['count'].iloc[4],
        count['count'].iloc[5],
        count['count'].iloc[6],
        count['count'].iloc[7],
        count['count'].iloc[8],
        count['count'].iloc[9]]
explode  = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1 )
fig1, ax1 = plt.subplots(figsize=(10,10))
ax1.pie(
    size,
    explode = explode,
    labels = label,
    autopct = '%1.1f%%',
    shadow = True,
    startangle = 0
)

plt.title('Sepuluh Penerbit Teratas dengan Jumlah Buku Terbanyak')
ax1.axis('equal')
plt.show()

# Visualisasi Top 20 Publisher
top_20 = count.sort_values(by=['count'], ascending = False)
top_20 = top_20.head(20)
x =['Harlequin','Silhouette','Pocket','Ballantine Books','Bantam Books','Scholastic','Simon &amp; Schuster']
y = [7537,4220,3905,3783,3646,3160,2971]
fig=plt.figure(figsize=(10,7))
ax = sns.barplot(x = 'count',y = 'c' , data = top_20)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90,horizontalalignment='center')
plt.xlabel("No of Books Published", size=14)
plt.ylabel("Penerbit", size=14)
plt.title(" Dua Puluh Penerbit Teratas dengan Jumlah Buku Terbanyak", size=18)
for p in ax.patches:
    ax.annotate("%.0f" % p.get_width(), xy=(p.get_width()/2, p.get_y()+p.get_height()/2),
            xytext=(5, 0), textcoords='offset points', ha="left", va="center")
plt.show()

# Cek nilai unik dari Year_Of_Publication
np.set_printoptions(threshold=np.inf)
buku['Year_Of_Publication'].unique()

"""Terdapat nilai **0** dan **strings** pada kolom tahun"""

# Mencari posisi strings dan 0 pada Years_Of_Publications dan drop
index=buku.loc[buku['Year_Of_Publication']=='DK Publishing Inc'].index
buku.drop(index,inplace=True)

index=buku.loc[buku['Year_Of_Publication']=='Gallimard'].index
buku.drop(index,inplace=True)

buku['Year_Of_Publication'].replace({'0':buku['Year_Of_Publication'].value_counts().idxmax()},inplace=True)

buku['Year_Of_Publication'] = buku['Year_Of_Publication'].astype(str).astype(int)
buku['Year_Of_Publication'].unique()

# Visualisasi Year of Publication
fig=plt.figure(figsize=(8,5))
y1 = buku[buku['Year_Of_Publication'] >= 1960]
y1 = y1[y1['Year_Of_Publication'] <= 2005]
sns.distplot(y1['Year_Of_Publication'])
plt.xlabel('Tahun Terbit',size=14)
plt.title('Histogram Tahun Penerbitan',size=16)
plt.show()

"""### **Eksplorasi Dataset Pengguna**"""

# Cek info dataset pengguna
print(pengguna.shape)

# Cek info dataset pengguna
pengguna.columns=['UserID', 'Location', 'Age']
pengguna.head()

# Cek info dataset pengguna
pengguna.info()

# Cek Missing Value
P = ((pengguna.isnull().sum()).sort_values()).to_dict()
for x in P:
  print(x, ":", P[x])

# Handle missing value dengan fillna
pengguna['Age'].fillna(pengguna['Age'].value_counts().idxmax(),inplace=True)

for x in pengguna['Age'][pengguna['Age']>95]:
  pengguna['Age'].replace({x:pengguna['Age'].value_counts().idxmax()},inplace=True)

for x in pengguna['Age'][pengguna['Age']==0]:
  pengguna['Age'].replace({x:pengguna['Age'].value_counts().idxmax()}, inplace=True)

pengguna['Age'] = pengguna['Age'].astype(int)
pengguna['Age'].unique()

# Cek Missing Value
P = ((pengguna.isnull().sum()).sort_values()).to_dict()
for x in P:
  print(x, ":", P[x])

# Visualisasi Histogram Pada Umur Pengguna

figura = plt.figure(figsize=(8,5))
sns.distplot(pengguna['Age'])
plt.xlabel('Age', size=14)
plt.title('Histogram Umur Pengguna', size=16)
plt.show()

# Cek lokasi pengguna
pengguna['Location']

# Membuat dataframe pengguna baru dengan 9 kolom (7 kolom nan)
# Menerapkan fungsi lambda pada kolom Location,
# Merubah elemen menjadi string dengan str()
# Split string dengan comma, .split(",")
pengguna[['city', 'state', 'country', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan']]=pengguna['Location'].apply(lambda x: pd.Series(str(x).split(",")))

#Restruktur dataframe dengan drop kolom Location dan Nan
pengguna.drop(['Location', 'nan'],axis=1, inplace=True)
pengguna

"""Proses preprocessing data lokasi telah selesai dan data siap digunakan

Selanjutnya, preprocessing dan eksplorasi data rating

### **Rating**
"""

# Cek informasi data rating
print(nilai.shape)

# Cek informasi data rating
nilai.columns = ['UserID', 'ISBN', 'Rating']
nilai.head()

# Cek nilai unik pada data rating
nilai['Rating'].unique()

"""### **Merger Data Rating dan Pengguna Berdasarkan UserID**"""

# Filter data rating pada kolom UserID yg hanya tersedia di data pengguna pada kolom UserID
filter_UserID = nilai[nilai['UserID'].isin(pengguna['UserID'])]

# Filter data pada filter_UserID pada kolom ISBN harus tersedia pada data buku pada kolom ISBN juga
filter_ISBN = filter_UserID[filter_UserID['ISBN'].isin(buku['ISBN'])]

#Merger data pengguna dengan data filter_ISBN
df = pd.merge(pengguna, filter_ISBN, on='UserID')

# Tampilkan df
df

# Visualisasi UserID dengan rating tertinggi
# Visualisasi dengan Pie Chart
my_data = (filter_ISBN['Rating'].value_counts()).to_dict()
count = pd.DataFrame(list(my_data.items()), columns = ['c', 'count'])
b = count.sort_values(by=['count'], ascending = False)
b.head(5)

labels = 'UserID: 153662','UserID: 11671','UserID: 98391','UserID: 198711','UserID: 35859'
sizes = [count['count'].iloc[0],
         count['count'].iloc[1],
         count['count'].iloc[2],
         count['count'].iloc[3],
         count['count'].iloc[4]]
explode = (0.1, 0.1, 0.1, 0.1, 0.1)
fig1, ax1 = plt.subplots(figsize=(5,5))
ax1.pie(sizes,
        explode = explode,
        labels = labels,
        autopct = '%1.1f%%',
        shadow = True,
        startangle = 0)

plt.title('UserIDs dengan Rating Tertinggi')
ax1.axis('equal')
plt.show()

# Visualisasi negara dengan user terbanyak
my_data = (pengguna['country'].value_counts()).to_dict()
count = pd.DataFrame(list(my_data.items()), columns=['c', 'count'])
n = count.sort_values(by = ['count'], ascending = False)
n = n.head(7)
fig = plt.figure(figsize = (10,5))
ax = sns.barplot(y = 'count', x = 'c', data = n)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90,horizontalalignment='center')
for bar in ax.patches:
    ax.annotate(format(bar.get_height(), '.0f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=8, xytext=(0,8),
                   textcoords='offset points')
plt.xlabel("Negara", size=14)
plt.ylabel("Jumlah Pengguna", size=14)
plt.title("Tujuh Negara dengan Pengguna Terbanyak", size=18)
plt.show()

# Visualisasi Negara dengan Jumlah User Terbanyak
my_dict=(pengguna['country'].value_counts()).to_dict()
count= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])
a = count.sort_values(by=['count'], ascending = False)
a.head(7)
labels = 'United Kingdom','Australia','USA','Germany','Italy','Canada','Spain'
sizes = [count['count'].iloc[2],count['count'].iloc[5],count['count'].iloc[0],count['count'].iloc[3],count['count'].iloc[6],
         count['count'].iloc[1],count['count'].iloc[4]]
explode = (0.1, 0.1, 0.1, 0.1,0.1, 0.1,0.1 )

fig1 , ax1 = plt.subplots(figsize=(7,7))

ax1.pie(sizes,
        explode = explode,
        labels = labels,
        autopct = '%1.1f%%',
        shadow = True,
        startangle = 0)
plt.title("Tujuh Negara dengan Pengguna Terbanyak")
ax1.axis ('equal')

plt.show()

pip install pycountry

# Ekstrak nama negara unik dari dataframe
coun=[]
for country in df["country"].unique():
    coun.append(country)

# Use the pycountry library to map country names to their corresponding 3-letter ISO codes,
# handling potential errors gracefully.
import pycountry
def do_fuzzy_search(country):
    result = pycountry.countries.search_fuzzy(country)
    return result[0].alpha_3

# Create a new DataFrame that contains the country names
# and their associated ISO codes (or 'unknown' for those not found).
iso_map=[]
c=[]
for i in coun:
    try:
        iso_map.append(do_fuzzy_search(i))
        c.append(i)
    except:
        iso_map.append('unknown')
        c.append(i)
        continue
df1=pd.DataFrame(iso_map,c,columns=['code'])
df1

l=list(df1.index)
country_code=[]
for i in df['country']:
    if i in l:
        country_code.append(df1['code'].loc[df1.index==i][0])
df['Country_Code'] = np.array(country_code)

import pycountry
grouped = df.groupby(['Country_Code','country'])
avg=pd.DataFrame(grouped['Rating'].agg(np.mean))
avg.reset_index(inplace=True)
avg.columns=['Country Code','Country','Average Rating']
import plotly.express as px
fig = px.choropleth(avg,
                    locations=avg['Country Code'],
                    color=avg['Average Rating'],
                    hover_name=avg['Country'],
                    color_continuous_scale=px.colors.sequential.Plasma)
fig.show()

import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
import re
from PIL import Image
import requests
import random
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords

"""# **Sistem Rekomendasi Berdasarkan Popularitas Buku**

+ Sistem rekomendasi ini berdasarkan rating buku yang diberikan oleh pengguna
+ Sistem rekomendasi ini menunjukkan tren buku yang disukai pengguna saat ini
"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

# Load data baru
buku_baru = pd.read_csv('/content/Books.csv')
user_baru = pd.read_csv('/content/Users.csv')
rating_baru = pd.read_csv('/content/Ratings.csv')

"""### **Data Preprocessing**"""

# Merger dataframe rating dengan buku berdasarkan ISBN
rating_buku = pd.merge(
    rating_baru,
    buku_baru,
    on='ISBN',
    how='left'
)
rating_buku

# Grouping dataframe rating_buku berdasarkan ISBN
rating_buku.groupby('ISBN').sum()

"""# **Sistem Rekomendasi: Content-Based Filtering**"""

# Cek missing value pada rating_buku
rating_buku.isnull().sum()

# Drop missing value
rating_buku_bersih = rating_buku.dropna()
rating_buku_bersih

# Cek missing value pada data bersih
rating_buku_bersih.isnull().sum()

# Standarisasi data
std_rating_buku = rating_buku_bersih.sort_values('ISBN', ascending=True)
std_rating_buku

# Cek panjang data unik pada ISBN
len(std_rating_buku['ISBN'].unique())

# Cek panjang data unik pada Book-Title
len(std_rating_buku['Book-Title'].unique())

data_fix = std_rating_buku.drop_duplicates('ISBN')
data_fix

# Konversi data ISBN ke list
data_isbn = data_fix['ISBN'].tolist()

# Konversi data Book-Title ke list
data_book_title = data_fix['Book-Title'].tolist()

# Konversi data Book-Author ke list
data_book_author = data_fix['Book-Author'].tolist()

# Konversi data Year-Of-Publication ke list
data_Year = data_fix['Year-Of-Publication'].tolist()

# Konversi data publisher ke list
data_publisher = data_fix['Publisher'].tolist()

print(len(data_isbn))
print(len(data_book_title))
print(len(data_book_author))
print(len(data_Year))
print(len(data_publisher))

# Dataframe baru
data_buku_fix = pd.DataFrame({
    'ISBN':data_isbn,
    'Book_Title':data_book_title,
    'Book_Author':data_book_author,
    'Year_Of_Publication':data_Year,
    'Publisher':data_publisher
})
data_buku_fix

# Hanya menggunakan 20.000 data karena terlalu besar
data_buku_fix = data_buku_fix[:20000]

"""### **TF-IDF Vectorizer**"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisiasi TfidVectorizer
tfidvect = TfidfVectorizer()

# Implementasi TfidVectorizer pada data_buku_fix di kolom Book_Author
tfidvect.fit(data_buku_fix['Book_Author'])

tfidvect.get_feature_names_out()

# Transformasi ke matrix
tfidvect_matrix = tfidvect.fit_transform(data_buku_fix['Book_Author'])

# Cek shape
tfidvect_matrix.shape

"""### **Cosine Similarity**"""

from sklearn.metrics.pairwise import cosine_similarity

# Hitung cosine similarity pada tfidvect_matrix
cosine_sim = cosine_similarity(tfidvect_matrix)
cosine_sim

# Membuat dataframe dari cosine_sim
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_buku_fix['Book_Title'], columns=data_buku_fix['Book_Title'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### **Mendapatkan Rekomendasi**"""

def rekomen_buku(data_book_title, similarity_data=cosine_sim_df, items=data_buku_fix[['Book_Title', 'Book_Author']], k=5):

     index = similarity_data.loc[:,data_book_title].to_numpy().argpartition(range(-1, -k, -1))

     # Retrieve data with the greatest similarity from the existing index
     closest = similarity_data.columns[index[-1:-(k+2):-1]]

     # Drop book_title so that the name of the book you are looking for does not appear in the recommendation list
     closest = closest.drop(data_book_title, errors='ignore')

     return pd.DataFrame(closest).merge(items).head(k)

book_title_tes = "Johnny Panic and the Bible of Dreams: Short Stories, Prose, and Diary Excerpts" # book title example

data_buku_fix[data_buku_fix.Book_Title.eq(book_title_tes)]

# Tes Rekomendasi yg sudah dibuat
rekomen_buku(book_title_tes)

"""### **Evaluasi**"""

# Tentukan threshold untuk kategorisasi similarity = 1 or 0
threshold = 0.5

# Buat ground truth data dengan threshold asumsi 1 or 0
ground_truth_data = np.where(cosine_sim >= threshold, 1, 0)

# Tampilkan nilai di ground truth data matrix
ground_truth_df = pd.DataFrame(ground_truth_data, index=data_buku_fix['Book_Title'],
                               columns=data_buku_fix['Book_Title']).sample(5, axis=1).sample(10, axis=0)

ground_truth_df

from sklearn.metrics import precision_recall_fscore_support

# Mengambil sebagian kecil dari matriks kemiripan kosinus dan matriks ground truth
sample_size = 10000
cosine_sim_sample = cosine_sim[:sample_size, :sample_size]
ground_truth_sample = ground_truth_data[:sample_size, :sample_size]

# Mengonversi matriks kemiripan kosinus menjadi larik satu dimensi untuk perbandingan
cosine_sim_flat = cosine_sim_sample.flatten()

# Mengubah matriks kebenaran dasar menjadi larik satu dimensi
ground_truth_flat = ground_truth_sample.flatten()

# Menghitung metrics evaluasi
predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
     ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# **Sistem Rekomendasi: Collaborative Filtering**"""

# Hanya menggunakan 20000 data poin
rating_fix = rating_baru[:20000]
rating_fix

# Konversi User-ID ke list
USER_ID = rating_fix['User-ID'].unique().tolist()
print('List User-ID: ', USER_ID)

# Konversi User-ID encoding
user_encoding = {x: i for i, x in enumerate(USER_ID)}
print('Hasil Encoding User-ID: ', user_encoding)

# Proses encoding ke User-ID
encoding_user = {i: x for i, x in enumerate(USER_ID)}
print('Encoded number ke User-ID: ', encoding_user)

# convert ISBNs ke list tanpa matching values
isbn_id = rating_fix['ISBN'].unique().tolist()

# Lakukan ISBN encoding
isbn_encoded = {x: i for i, x in enumerate(isbn_id)}

# Proses of encoding numbers to ISBN
encoded_isbn = {i: x for i, x in enumerate(isbn_id)}

# Nonaktifkan warning messages
pd.options.mode.chained_assignment = None # "warn" or "raise" to turn it back on

# Mapping User-ID ke user dataframe
rating_fix['user'] = rating_fix['User-ID'].map(user_encoding)

# Mapping ISBN ke book title dataframe
rating_fix['book_title'] = rating_fix['ISBN'].map(isbn_encoded)

# Dapatkan jumlah user
num_users = len(user_encoding)
print(num_users)

# Dapatkan jumlah judul buku
num_book_title = len(isbn_encoded)
print(num_book_title)

# convert the rating to a float value
rating_fix['Book-Rating'] = rating_fix['Book-Rating'].values.astype(np.float32)

# minimum rating value
min_rating = min(rating_fix['Book-Rating'])

# maximum rating value
max_rating = max(rating_fix['Book-Rating'])

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
     num_users, num_book_title, min_rating, max_rating
))

# Bagi data training dan validasi
rating_fix = rating_fix.sample(
    frac = 1,
    random_state = 42
)

rating_fix

# buat variabel x to match user data dan book title menjadi satu nilai
x = rating_fix[['user', 'book_title']].values

# create a y variable to create a rating of the results
y = rating_fix['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# divide into 90% train data and 10% validation data

train_indices = int(0.9 * rating_fix.shape[0])
x_train, x_val, y_train, y_val = (
     x[:train_indices],
     x[train_indices:],
     y[:train_indices],
     y[train_indices:]
)

print(x, y)

# Proses Training

class RecommenderNet(tf.keras.Model):

     # function initialization
     def __init__(self, num_users, num_book_title, embedding_size, dropout_rate=0.2, **kwargs):
         super(RecommenderNet, self).__init__(**kwargs)
         self.num_users = num_users
         self.num_book_title = num_book_title
         self. embedding_size = embedding_size
         self.dropout_rate = dropout_rate

         self.user_embedding = layers.Embedding( # user embedding layer
             num_users,
             embedding_size,
             embeddings_initializer = 'he_normal',
             embeddings_regularizer =keras.regularizers.l2(1e-6)
         )
         self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

         self.book_title_embedding = layers.Embedding( # book_title embedding layer
             num_book_title,
             embedding_size,
             embeddings_initializer = 'he_normal',
             embeddings_regularizer =keras.regularizers.l2(1e-6)
         )
         self.book_title_bias = layers.Embedding(num_book_title, 1) # layer embedding book_title

         self.dropout = layers.Dropout(rate=dropout_rate)

     def call(self, inputs):
         user_vector = self.user_embedding(inputs[:, 0]) # call embedding layer 1
         user_vector = self.dropout(user_vector)
         user_bias = self.user_bias(inputs[:, 0]) # call embedding layer 2

         book_title_vector = self.book_title_embedding(inputs[:, 1]) # call embedding layer 3
         book_title_vector = self.dropout(book_title_vector)
         book_title_bias = self.book_title_bias(inputs[:, 1]) # call embedding layer 4

         dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2) # dot product multiplication

         x = dot_user_book_title + user_bias + book_title_bias

         return tf.nn.sigmoid(x) # activate sigmoid

model = RecommenderNet(num_users, num_book_title, 50) # initialize model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-4),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

# start the training process

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""### **Mendapatkan Rekomendasi**"""

book_df = data_buku_fix

# Ambil sampel pengguna
user_id = rating_fix['User-ID'].sample(2).iloc[0]
book_readed_by_user = rating_fix[rating_fix['User-ID'] == user_id]

# buat variabel book_not_readed
book_not_readed = book_df[~book_df['ISBN'].isin(book_readed_by_user['ISBN'].values)]['ISBN']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_encoded.keys()))
)

book_not_readed = [[isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_encoding.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    encoded_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]

top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_df[book_df['ISBN'].isin(top_book_user)]

# Displays book recommendations in DataFrame form
book_df_rows_data = []
for row in book_df_rows.itertuples():
    book_df_rows_data.append([row.Book_Title, row.Book_Author])

recommended_book = book_df[book_df['ISBN'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.Book_Title, row.Book_Author])

# Create a DataFrame for output
output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_df_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Displays recommendation results in DataFrame form
print("Showing recommendation for users: {}".format(user_id))
print("---" * 9)
print("Book with high ratings from user")
print("..." * 8)
print(df_book_readed_by_user)
print("..." * 8)
print("Top 10 books recommendation")
print("..." * 8)
df_recommended_books

"""### **Evaluasi**"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Metrik Evaluasi')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()